{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a22c1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential,layers,datasets,optimizers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bafafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The most frequent words\n",
    "total_words = 10000\n",
    "(x_train,y_train),(x_test,y_test) = datasets.imdb.load_data(num_words = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267f95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length(padding )\n",
    "max_reveiw_len = 80 \n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_reveiw_len)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_reveiw_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae8a3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54576ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 128\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "db_train = db_train.shuffle(1000).batch(batch_size=batchsz,drop_remainder=True) #drop the last batch\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \n",
    "db_test = db_test.batch(batch_size=batchsz,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d327a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 80])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one batch shape\n",
    "next(iter(db_train))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5deb98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn cell\n",
    "class MyRNN(keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(MyRNN,self).__init__()\n",
    "        self.state0 = [tf.zeros([batchsz,units])]\n",
    "        self.state1 = [tf.zeros([batchsz,units])]\n",
    "        # transform text to embedding representation\n",
    "        # [b,80] -> [b,80,100]\n",
    "        self.embedding = layers.Embedding(total_words,100,input_length = max_reveiw_len)\n",
    "        # [b,80,100]  h_dim: 64\n",
    "        self.rnn_cell0 = layers.SimpleRNNCell(units,dropout = 0.2)\n",
    "        self.rnn_cell1 = layers.SimpleRNNCell(units,dropout = 0.2)\n",
    "        # [b,80,100] -> [b,64] -> [b,1] \n",
    "        self.outlayer = layers.Dense(1)\n",
    "    def call(self,inputs,training = None):\n",
    "        x = inputs\n",
    "        #[b,80] -> [b,80,100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn compute\n",
    "        # [b,80,100] -> [b,64]\n",
    "        state0 = self.state0\n",
    "        state1 = self.state1\n",
    "        for word in tf.unstack(x,axis=1): # word: [b,100]\n",
    "            # h1 = x*wxh +h*whh\n",
    "            out,state0 = self.rnn_cell0(word,state0,training)\n",
    "            out,state1 = self.rnn_cell1(out,state1)\n",
    "          #out: [b,64]\n",
    "        x = self.outlayer(out)\n",
    "        #p(y id pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b89d0037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "class MyRNN(keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(MyRNN,self).__init__()\n",
    "        self.state0 = [tf.zeros([batchsz,units]),tf.zeros([batchsz,units])]\n",
    "        self.state1 = [tf.zeros([batchsz,units]),tf.zeros([batchsz,units])]\n",
    "        # transform text to embedding representation\n",
    "        # [b,80] -> [b,80,100]\n",
    "        self.embedding = layers.Embedding(total_words,100,input_length = max_reveiw_len)\n",
    "        # [b,80,100]  h_dim: 64\n",
    "        self.rnn_cell0 = layers.LSTMCell(units,dropout = 0.5)\n",
    "        self.rnn_cell1 = layers.LSTMCell(units,dropout = 0.5)\n",
    "        # [b,80,100] -> [b,64] -> [b,1] \n",
    "        self.outlayer = layers.Dense(1)\n",
    "    def call(self,inputs,training = None):\n",
    "        x = inputs\n",
    "        #[b,80] -> [b,80,100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn compute\n",
    "        # [b,80,100] -> [b,64]\n",
    "        state0 = self.state0\n",
    "        state1 = self.state1\n",
    "        for word in tf.unstack(x,axis=1): # word: [b,100]\n",
    "            # h1 = x*wxh +h*whh\n",
    "            out,state0 = self.rnn_cell0(word,state0,training)\n",
    "            out,state1 = self.rnn_cell1(out,state1)\n",
    "          #out: [b,64]\n",
    "        x = self.outlayer(out)\n",
    "        #p(y id pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "539d71af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "195/195 [==============================] - 37s 123ms/step - loss: 0.5127 - accuracy: 0.7288 - val_loss: 0.3748 - val_accuracy: 0.8335\n",
      "Epoch 2/4\n",
      "195/195 [==============================] - 21s 107ms/step - loss: 0.3239 - accuracy: 0.8637 - val_loss: 0.3710 - val_accuracy: 0.8360\n",
      "Epoch 3/4\n",
      "195/195 [==============================] - 21s 107ms/step - loss: 0.2752 - accuracy: 0.8895 - val_loss: 0.3725 - val_accuracy: 0.8369\n",
      "Epoch 4/4\n",
      "195/195 [==============================] - 21s 107ms/step - loss: 0.2381 - accuracy: 0.9074 - val_loss: 0.4731 - val_accuracy: 0.8236\n",
      "195/195 [==============================] - 6s 32ms/step - loss: 0.4731 - accuracy: 0.8236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47312843799591064, 0.823637843132019]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = 64\n",
    "epochs = 4\n",
    "model = MyRNN(units)\n",
    "model.compile(optimizer=optimizers.Adam(0.001),\n",
    "             loss = tf.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.fit(db_train,epochs = epochs, validation_data=db_test)\n",
    "model.evaluate(db_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468af644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer method\n",
    "class MyRNN(keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(MyRNN,self).__init__()\n",
    "      \n",
    "        # transform text to embedding representation\n",
    "        # [b,80] -> [b,80,100]\n",
    "        self.embedding = layers.Embedding(total_words,100,input_length = max_reveiw_len)\n",
    "        # [b,80,100]  h_dim: 64\n",
    "        self.rnn = keras.Sequential([\n",
    "#             layers.SimpleRNN(units,dropout = 0.5,return_sequences = True, unroll = True),\n",
    "#             layers.SimpleRNN(units,dropout = 0.5,unroll = True)\n",
    "            \n",
    "#             layers.LSTM(units,dropout = 0.5,return_sequences = True, unroll = True)\n",
    "#             layers.LSTM(units,dropout = 0.5,unroll = True)\n",
    "            layers.GRU(units,dropout = 0.5,return_sequences = True, unroll = True),\n",
    "            layers.GRU(units,dropout = 0.5,unroll = True)\n",
    "        ])\n",
    "        \n",
    "        # [b,80,100] -> [b,64] -> [b,1] \n",
    "        self.outlayer = layers.Dense(1)\n",
    "    def call(self,inputs,training = None):\n",
    "        x = inputs\n",
    "        #[b,80] -> [b,80,100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn compute\n",
    "        # [b,80,100] -> [b,64]\n",
    "        x = self.rnn(x)\n",
    "          #out: [b,64]\n",
    "        x = self.outlayer(x)\n",
    "        #p(y id pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7dc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
